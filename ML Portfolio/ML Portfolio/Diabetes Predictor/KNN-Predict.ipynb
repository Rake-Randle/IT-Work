{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Predict Diabetes with K-NN\n",
                "\n",
                "### Objective: Predict whether a person will be diagnosed with diabetes or not\n",
                "\n",
                "- Dataset of 768 diagnosed people with or without diabetes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import f1_score\n",
                "from sklearn.metrics import accuracy_score"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## K-NN Algorithm Explanation\n",
                "\n",
                " <table>\n",
                "  <tr>\n",
                "    <td>\n",
                "      <img src=\"KNN_graph1.png\" width=\"200\" />\n",
                "    </td>\n",
                "        <td>\n",
                "      <img src=\"KNN_graph3.png\" width=\"245\" />\n",
                "    </td>\n",
                "    <td>\n",
                "      <img src=\"KNN_graph2.png\" width=\"245\" />\n",
                "    </td>\n",
                "  </tr>\n",
                "</table>\n",
                "\n",
                "- Group items by similar characteristics\n",
                "- K - the **hyperparameter**, the number of voting members\n",
                "- K (Number of Neighbors): The 'K' hyperparameter defines the number of nearest neighbors to consider when making predictions for a new data point. For example, if k=3, the algorithm will consider the three nearest neighbors to the query point and make a prediction based on the majority class among those three neighbors (for classification tasks). The choice of 'k' can significantly impact the algorithm's performance.\n",
                "\n",
                "Choosing the right value of 'k' is crucial because:\n",
                "\n",
                "- A small 'K' (e.g., k=1) can lead to noise sensitivity and overfitting. The algorithm might be highly influenced by outliers or noise in the data, leading to poor generalization to new data points.\n",
                "- A large 'K' (e.g., k=20) can lead to oversmoothing and underfitting. The algorithm may lose the ability to capture local patterns in the data, resulting in a more biased model."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Pre-processing\n",
                "\n",
                "- Values for 'Glucose' and 'Blood Pressure' etc. cannot be accepted as zeros (why?)\n",
                "- Replace these columns with the mean of the column\n",
                "- Replace with average since it is the most common for a person"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data = pd.read_csv(\"diabetes.csv\") # Pushes data into a variable\n",
                "print(len(data))\n",
                "print(data.head()) # Prints first 5 rows of data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Replace the zeros in the columns \n",
                "zero_not_accepted = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
                "\n",
                "# Looping through the datam, replacing the zeros with the column mean/average\n",
                "for column in zero_not_accepted: \n",
                "    data[column] = data[column].replace(0, np.NaN)\n",
                "    mean = int(data[column].mean(skipna=True))\n",
                "    # data[column] = data[column].replace(np.NaN, mean) # Use this or\n",
                "    data[column].fillna(mean, inplace=True) # This line, both work"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split the data sets for training and testing\n",
                "x = data.iloc[:, 0:8] # Keeps all rows, exclude column 8 \n",
                "y = data.iloc[:, 8] # Keep all rows, include only column 8 \n",
                "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0, test_size=0.38)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Attribute scaling\n",
                "sc_x = StandardScaler() # Sets all data between values -1 and 1\n",
                "x_train = sc_x.fit_transform(x_train)\n",
                "x_test = sc_x.transform(x_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Determining the \n",
                "### Determining the model variables\n",
                "\n",
                "- The hyperparameter K should be an odd number (why?)\n",
                "- Use the square root of the length of the y_test set to pick K\n",
                "- p is the 'diabetic or not'\n",
                "- Euclidean is the method of measuring the distance between neighbors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Determine the K value (the hyperparameter)\n",
                "import math\n",
                "print(len(y_test))\n",
                "math.sqrt(len(y_test))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the model : Initiate the K-NN\n",
                "classifier = KNeighborsClassifier(n_neighbors=17, p=2, metric='euclidean')\n",
                "classifier.fit(x_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict the test results\n",
                "y_pred = classifier.predict(x_test)\n",
                "y_pred # Prints all predictions"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The Confusion Matrix Analysis Tool\n",
                "\n",
                "In machine learning, the confusion matrix is a table that is used to evaluate the performance of a classification model, such as a K-Nearest Neighbors (KNN) model. It provides a comprehensive summary of the predictions made by the model on a test dataset, compared to the actual ground truth labels. The confusion matrix has four important metrics: True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).\n",
                "\n",
                "1. True Positives (TP): The number of instances that are correctly predicted as positive (belong to the positive class).\n",
                "\n",
                "2. False Positives (FP): The number of instances that are incorrectly predicted as positive but actually belong to the negative class.\n",
                "\n",
                "3. True Negatives (TN): The number of instances that are correctly predicted as negative (belong to the negative class).\n",
                "\n",
                "4. False Negatives (FN): The number of instances that are incorrectly predicted as negative but actually belong to the positive class.\n",
                "\n",
                "<table>\n",
                "    <tr>\n",
                "        <th>Actual</th>\n",
                "        <th>Predicted Positive</th>\n",
                "        <th>Predicted Negative</th>\n",
                "    </tr>\n",
                "    <tr>\n",
                "        <td>Actual Positive </td>\n",
                "        <td>TP</td>\n",
                "        <td>FN</td>\n",
                "    </tr>\n",
                "    <tr>\n",
                "        <td>Actual Negative </td>\n",
                "        <td>FP</td>\n",
                "        <td>TN</td>\n",
                "    </tr>\n",
                "</table>\n",
                "\n",
                "Here's how to interpret the confusion matrix:\n",
                "\n",
                "1. TP (True Positives): The model correctly predicted instances that belong to the positive class.\n",
                "\n",
                "2. FN (False Negatives): The model incorrectly predicted instances as negative when they actually belong to the positive class.\n",
                "\n",
                "3. FP (False Positives): The model incorrectly predicted instances as positive when they actually belong to the negative class.\n",
                "\n",
                "4. TN (True Negatives): The model correctly predicted instances that belong to the negative class.\n",
                "\n",
                "Using these values, we can compute various evaluation metrics, such as **accuracy**, precision, recall (sensitivity), specificity, and **F1-score**, which help us assess the performance of the KNN model on the test data.\n",
                "\n",
                "- Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
                "- Precision = TP / (TP + FP)\n",
                "- Recall (Sensitivity) = TP / (TP + FN)\n",
                "- Specificity = TN / (TN + FP)\n",
                "- F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
                "\n",
                "A good KNN model will have a high accuracy, precision, recall, specificity, and F1-Score, indicating that it is making correct predictions for both positive and negative classes.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate the model  \n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "print(cm)\n",
                "print(f1_score(y_test, y_pred))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### To interpret the results in the confusion matrix \n",
                "[[94, 13], \n",
                "[15, 32]]\n",
                "\n",
                "```\n",
                "             Predicted Positive    Predicted Negative\n",
                "Actual Positive        94                13\n",
                "Actual Negative        15                32\n",
                "```\n",
                "\n",
                "Here's how to interpret the values:\n",
                "\n",
                "1. True Positives (TP): The number of instances that are correctly predicted as positive. In this case, there are 94 true positives, meaning 94 instances were correctly classified as positive (belong to the positive class).\n",
                "\n",
                "2. False Positives (FP): The number of instances that are incorrectly predicted as positive but actually belong to the negative class. In this case, there are 13 false positives, meaning 13 instances were incorrectly classified as positive when they actually belong to the negative class.\n",
                "\n",
                "3. True Negatives (TN): The number of instances that are correctly predicted as negative. In this case, there are 32 true negatives, meaning 32 instances were correctly classified as negative (belong to the negative class).\n",
                "\n",
                "4. False Negatives (FN): The number of instances that are incorrectly predicted as negative but actually belong to the positive class. In this case, there are 15 false negatives, meaning 15 instances were incorrectly classified as negative when they actually belong to the positive class.\n",
                "\n",
                "Based on these values, we can calculate various evaluation metrics to assess the performance of the classification model:\n",
                "\n",
                "1. Accuracy: It is the overall correctness of the model's predictions and is calculated as (TP + TN) / (TP + FP + TN + FN). In this case, accuracy = (94 + 32) / (94 + 13 + 15 + 32) ≈ 0.8095 or 80.95%.\n",
                "\n",
                "2. Precision: It measures the accuracy of the positive predictions and is calculated as TP / (TP + FP). In this case, precision = 94 / (94 + 13) ≈ 0.8785 or 87.85%.\n",
                "\n",
                "3. Recall (Sensitivity): It measures the ability of the model to correctly identify positive instances and is calculated as TP / (TP + FN). In this case, recall = 94 / (94 + 15) ≈ 0.8621 or 86.21%.\n",
                "\n",
                "4. F1-Score: It is the harmonic mean of precision and recall and is given by 2 * (Precision * Recall) / (Precision + Recall). In this case, F1-Score = 2 * (0.8785 * 0.8621) / (0.8785 + 0.8621) ≈ 0.8702 or 87.02%.\n",
                "\n",
                "Our model has a relatively good accuracy, precision, recall, and F1-Score, indicating it performs reasonably well on the given test dataset. However, further analysis and comparison with other models may be necessary for a comprehensive evaluation."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
